\section{Related Work}
\label{sec:related}

Workload-aware, lossless matrix compression is related to lossless and lossy matrix compression,
query processing on compressed data, and workload-aware physical design of compressed data.

% lossless compression (general purpose, CLA, )
\textbf{Lossless Matrix Compression:}
Naturally, the closest area of related work is lossless matrix compression, whose limitations we already discussed in Section~\ref{sec:background}. General-purpose data-parallel frameworks like Spark \cite{ZahariaCDDMMFSS12} or Flink \cite{AlexandrovBEFHHKLLMNPRSSHTW14}, scientific data formats like NetCDF and HDF5 \cite{hdf5}, and storage managers like SciDB \cite{StonebrakerBPR11} and TileDB \cite{PapadopoulosDMM16} also support compression, but decompress block- or partition-wise for operations. Early work includes traditional sparse matrix representations (e.g., CSR, CSC, COO) \cite{Saad94sparskit} and compression techniques by Kourtis et al. that already leveraged dictionary coding \cite{KourtisGK08, KarakasisGKGK13}, as well as delta and run-length encoding \cite{KarakasisGKGK13}.
Subsequent work on compressed linear algebra (CLA) focused on online compression and entire ML algorithms, where CLA \cite{ElgoharyBHRR16,ElgoharyBHRR18} uses column compression for batch algorithms, and TOC \cite{LiCZ00NP19} uses tuple compression for mini-batch algorithms.
Other works exploit different properties such as: integer time series values \cite{BlalockMG18}, floating point time series \cite{LiakosPK22}, and bounded ranges of floats \cite{LiuJPE21}.
Recent work on grammar-compressed matrices report operation performance proportional to the compressed size \cite{abs-2203-14540}, while others presented impossibility results (worst-case) for efficient matrix-vector multiplications on grammar-compressed matrices such as Lempel-Ziv \cite{AbboudBBK20}.
Factorized learning \cite{KumarNP15,SchleichOC16} further pushes operations of ML training algorithms through joins and can be seen as a specialized form of lossless compression exploiting available schema information \cite{Olteanu20} to avoid materializing denormalized tables. These factorization ideas can also be implemented on top of ML systems \cite{ChenKNP17} by representing joins via structured selection matrices. Compared to these mostly data-centric compression frameworks---where LMFAO \cite{SchleichOK0N19} also compiles efficient sum-product plans for factorized learning---our \name\ framework leverages both data and workload characteristics of linear algebra programs and adjusts the compression process, compressed representations, and execution plans accordingly.

\textbf{Lossy Compression and Sampling:} In the context of mini-batch DNN training and scoring, we see broad adoption of lossy compression. First, quantization discretizes floating-point into fixed-point representations such as \texttt{UINT8} for scoring \cite{tfmopt2}. Common techniques are static min/max binning (equi-width) \cite{tfmopt2} and learned quantization schemes (equi-height via quantiles) \cite{Zhang0KALZ17,ZhuHMD17}. Such quantization schemes are also used for efficient data transfer in ZipML \cite{Zhang0KALZ17} and SketchML \cite{JiangFY018}. With residual accumulation at the workers, some systems reduce communicated values to a single bit \cite{SeideFDLY14}. Second, the challenges of training with low 8-bit FP precision are addressed with chunk-based accumulation and stochastic rounding \cite{WangCBCG18}. Third, there are techniques like mantissa truncation \cite{AbadiBCCDDDGIIK16,BhattacherjeeDS14} and new data types with different trade-offs of range and precision \cite{A100}. Examples are Google's bfloat16 (1+8+7 bits) \cite{Saeta18}, Intel's Flexpoint (shared subset of exponent bits) \cite{KosterWWNBCEHHK17}, and NVIDIA's TF32 (1+8+10) \cite{A100}. Fourth, other techniques include sparsification or value clipping (omit small values) \cite{tfmopt1,A100}, dimensionality reduction like auto encoders \cite{IlkhechiCGMFSC20}, sampling in BlinkML \cite{ParkQSM19}, DNN activation compression in COMET \cite{JinZJFGLST2022}, and progressive compression schemes \cite{WangKZAZM19, KuchnikAS21}. Unfortunately, the unknown impact on results, creates trust concerns, requires trial and error, and is problematic for declarative ML pipelines. Recent work in MLWeaving \cite{WangKZAZM19} introduced data structures for efficiently extracting different granularities for simplifying exploration, while BlinkML \cite{ParkQSM19} estimates the minimum sample size to satisfy an accuracy constraint. Our work on lossless compression is orthogonal as it guarantees correct results.

\textbf{Workload-aware Physical Database Design:} Work on lossless matrix compression like CLA \cite{ElgoharyBHRR16,ElgoharyBHRR18} and TOC \cite{LiCZ00NP19} was inspired by lossless compression in column stores and related query processing on compressed data. There is a wide variety of lightweight lossless data compression schemes such as null suppression, run-length encoding, dictionary coding, frame of reference, and delta coding \cite{AbadiMF06,AbadiBH09}. Extensions include patched encoding schemes (separate handling of exception values) \cite{ZukowskiHNB06}, order-preserving dictionary coding \cite{BinnigHF09,LiuUJSME19}, and exploitation of such schemes in query processing \cite{RamanABCKKLLLLMMPSSSSZ13,BinnigHF09,LangMFB0K16,AthanassoulisBS19}. Our handling of default values is also related to header compression in SAP HANA \cite{saphana} and fast-mode column adds in Teradata \cite{teradata}. The performance/compressed-size tradeoffs of existing schemes are, however, strongly data-dependent \cite{HollowayRSD07,DammeUHHL19}. For that reason, existing systems largely rely on conservative selection heuristics \cite{LangMFB0K16,AbadiMF06,AbadiBH09}, but there is also work on cost modeling \cite{DammeUHHL19,0001J19,CenKMK21}, and balancing query performance and storage size with different column group projections and encodings \cite{RamakrishnaCAJS}. Once compression choices are reflected in the costs, they influence what-if physical design tuning. Compression-aware design tuning \cite{KimuraNS11} showed how index compression can affect index selection choices, and learned partitioning schemes maximize partitioning pruning \cite{YangCWGLMLKA20} (e.g., via small materialized aggregates \cite{Moerkotte98}). Furthermore, recent work introduced memory-budget-constrained offline compression for selecting encoding schemes based on estimated costs and compression ratios \cite{Boissier22}, and related data partitioning across storage tiers \cite{LaschSLS21,0001RIL0K20}. In contrast, our workload-aware compression planning summarizes the workload of a linear algebra program in order to tune \emph{online} lossless matrix compression and compressed operations. 



\section{Related Work}
\label{sec:related}

Our fine-grained, multi-level operation lineage tracing and reuse is related to traditional data provenance, model management, reuse of query intermediates and intermediates in ML pipelines, as well as the incremental maintenance of intermediates in ML systems.

\textbf{Data Provenance:} Data provenance for tracking the origin and creation of data has been extensively studied in the data management literature \cite{GlavicD07,CheneyCT09,Tan07}. Common types of data provenance are (1) why-provenance \cite{BunemanKT01,CuiWW00} (via input tuple ``witnesses''), (2) how-provenance \cite{GreenKIT07,GreenKT07} (via provenance polynomials), and (3) why-not-provenance (why eliminated) \cite{ChapmanJ09}. From an execution perspective, we distinguish eager and lazy provenance, which obtain lineage during execution or on demand \cite{CheneyCT09,PsallidasW18}. Furthermore, there is also work on fine-grained, tuple-oriented data provenance in data flow programs such as MapReduce \cite{DeanG04}, Spark \cite{ZahariaCDDMMFSS12}, or PigLatin \cite{OlstonRSKT08}. Examples are RAMP \cite{IkedaPW11}, HadoopProv \cite{AkoushSH13}, and Newt \cite{LogothetisDY13} for MapReduce, Lipstick \cite{AmsterdamerDDMST11} for PigLatin, and Titian \cite{InterlandiSTGYK15} for Spark. Similar to LIMA, the Lipstick system \cite{AmsterdamerDDMST11} uses a lineage graph for fine-grained provenance in a hierarchy of Pig Latin modules, but unlike LIMA, Lipstick operates at tuple granularity and does not deduplicate repeated structures. Provenance management in scientific workflows introduced additional ideas on user-centric provenance information \cite{DavidsonBELMBAF07, AnandBML09, LudascherABHJJLTZ06}, and caching \cite{BavoilCSVCSF05,CallahanFSSSV06}. Recent work focuses on fine-grained provenance for linear algebra via provenance polynomials on matrix partitions \cite{YanTI16}, efficient provenance tracking via RID-based indexes and query-aware optimizations \cite{PsallidasW18}, provenance for ETL workflows and entity resolution via how-provenance \cite{ZhengAI19}, provenance for data citation \cite{WuADMD19} and natural language claims \cite{ZhangIR20}, and provenance for blockchains via Merkle DAGs \cite{Ruan0DLOZ19}. In contrast to tuple-oriented provenance, we perform logical lineage tracing of linear algebra operations, operate at multiple levels of conditional control flow, and primarily focus on efficient lineage tracing for both reproducibility and reuse.

\textbf{Dataset and Model Management:} Recent work on catalogs for dataset and model management includes Google Goods \cite{HalevyKNOPRW16}, SAP Data Hub \cite{SAPDataHub}, and DataHub \cite{BhardwajBCDEMP15}, but also data market platforms \cite{FernandezSF20}, which all store provenance information to track the origins and preparation of datasets. In the context of open science, similar data catalogs are established under the FAIR data principles \cite{fair2016}, where principle R1.2 requires that ``(meta)data are associated with detailed provenance''. Related projects like Apache Atlas \cite{atlas}---as used in Microsoft's Enterprise ML vision \cite{AgrawalCCFGIJKK20}---offers APIs for storing provenance form different systems. DataHub \cite{BhardwajBCDEMP15} and DEX \cite{ChavanD17} further provided git-like dataset versioning with delta encoding; MISTIQUE \cite{VartakTMZ18} extended this line of work by lossy deduplication, compression, and adaptive materialization. Further related work includes ML model versioning, experiment tracking, and model management. For example, van der Weide et al. manually version ML pipeline functions, manage their pipeline dependencies, and reuse intermediates \cite{WeidePSZK17}. TensorFlow \cite{AbadiBCCDDDGIIK16} allows programmatic tracking of variables (e.g., loss) for experiment visualization in TensorBoard, while MLflow \cite{ZahariaCD0HKMNO18,ChenCDD0HKMMNOP20} provides APIs for tracking model parameters and resulting accuracy. Similarly, the ML library Tribuo~\cite{MachineL29:online} attaches provenance information to models and datasets for reproducibility. More specialized model management---like ModelHub \cite{MiaoLDD17a} and ModelDB \cite{VartakM18}---further provides model versioning and means of querying these model versions. In contrast to such coarse-grained versioning, we provide means of fine-grained lineage tracing of linear algebra programs \emph{inside} ML systems.

\textbf{Reuse of Query Intermediates:} There is also a long history on reusing work in database systems. The spectrum ranges from buffer pool page caching, over scan sharing \cite{UnterbrunnerGAFK09,ArumugamDJPP10}, request batching \cite{LeeZL07}, and adaptive indexing/cracking \cite{IdreosKM07}, to multi-query optimization \cite{RoySSB00} and materialized views \cite{AgrawalCN00,JindalKRP18}. Our work has been inspired by the seminal work on recycling intermediates in MonetDB \cite{IvanovaKNG09,IvanovaKNG10} and transient materialized views \cite{ZhouLFL07}. Both of these approaches leverage intermediates that are anyway materialized in memory (or materialized via \texttt{spool} operators) for future reuse. Accordingly, several aspects like runtime integration and cache eviction policies of our approach share similarities. However, in contrast to existing work, we provide means of efficient lineage tracing and reuse for \emph{linear algebra programs}, which entails dedicated loop deduplication strategies, multi-level lineage in conditional control flow, as well as linear-algebra-specific rewrites for full and partial reuse.

\textbf{Reuse of ML Pipeline Intermediates:} Exploratory data science workflows also have large reuse opportunities. Already notebooks---which preserve the state of cells---and extensions for dataset discovering \cite{ZhangI19} can be viewed as a form of manual reuse. Recent work leverages lineage tracing for notebook state-safety inspection \cite{nbsafety21, BrachmannSKGMCB20}. For example, NBSAFETY \cite{nbsafety21} provides a custom Jupyter kernel that highlights unsafe cell executions (via static analysis and runtime lineage tracing), but does not trace fine-grained lineage of library function calls. Early work on automated reuse was then introduced for optimizing ML pipelines in Columbus \cite{ZhangKR14} and KeystoneML \cite{SparksVKFR17}, which both reason about materialization and reuse for exact and approximate reuse, within and across pipelines. ML serving systems also apply means of reuse. Examples are function result caching in Clipper \cite{CrankshawWZFGS17} (e.g., caching frequently translated words), CSE across prediction programs in PRETZEL \cite{LeeSCSWI18}, and short-circuiting via reference labels in NoScope \cite{KangEABZ17}. More recent work include Alpine Meadow \cite{ShangZBKECBUK19}, HELIX \cite{XinMMLSP18}, and VAMSA \cite{NamakiFPKAWZW20}, which include operations for data preparation, ML training, and model evaluation. Similar to our compiler-assisted reuse and cache eviction policies, HELIX uses a cost model of load, materialization and computation costs, as well as a plan selection heuristic. Most of these systems rely on existing ML systems like Scikit-learn \cite{PedregosaVGMTGBPWDVPCBPD11} and reason about the pipeline DAG, which is a coarse-grained, top-level view of ML pipelines. In contrast, we exploit fine-grained full and partial reuse at multiple control flow granularities.

\textbf{Incremental Maintenance of ML Models:} Partial reuse of intermediates is closely related to the incremental maintenance of ML models and intermediates. Model serving systems like Velox \cite{CrankshawBGLZFG15} and classification Views in Hazy \cite{KocR11} apply online learning for model adaptation. In addition, there is also work on exact incremental maintenance---in linear algebra programs and related abstractions---such as LINVIEW \cite{NikolicEK14}, F-IVM \cite{NikolicO18}, and MauveDB \cite{DeshpandeM06}. An often exploited opportunity is maintaining $\mat{A} = \mat{X}^{\top}\mat{X}$ and $\mat{b}=\mat{X}^{\top}\mat{y}$, via $\mat{A}^{\prime} = \mat{A} + \Delta\mat{X}^{\top}\Delta\mat{X}$ and $\mat{b}^{\prime} = \mat{b} + \Delta\mat{X}^{\top}\Delta\mat{y}$. Recent work also leveraged this property for decremental updates \cite{Schelter20} (e.g., to remove tuples for GDPR regulations). Further work includes incremental grounding and inference in DeepDive \cite{ShinWWSZR15}, and incremental computation of occlusion-based explanations for CNNs \cite{NakandalaKP19,rh/Nakandala20}. In contrast to the incremental maintenance of intermediates, our partial reuse is more general because it allows rewrites to augment intermediates by complex compensation plans.

% reminder all differences
%Putting it altogether, our work combines efficient lineage tracing with techniques for full reuse (as known from recycling query and ML pipeline intermediates) and partial reuse (as partially know from incremental maintenance) into a practical framework for fine-grained lineage tracing of complex linear algebra programs.    

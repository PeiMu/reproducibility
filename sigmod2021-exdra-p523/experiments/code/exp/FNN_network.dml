#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------

# Imports
source("scripts/nn/layers/affine.dml") as affine
source("scripts/nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("scripts/nn/layers/relu.dml") as relu
source("scripts/nn/layers/softmax.dml") as softmax
source("scripts/nn/optim/sgd.dml") as sgd

train_paramserv = function(matrix[double] X, matrix[double] y,
                 int epochs, int workers,
                 string utype, string freq, int batch_size, string mode, double learning_rate, 
                 int seed = -1)
    return (list[unknown] model_trained) {

  N = nrow(X)  # num examples
  D = ncol(X)  # num features
  K = ncol(y)  # num classes

  X_val = matrix(0, rows = 0, cols = 0)
  y_val = matrix(0, rows = 0, cols = 0)
  # Create the network:
  ## input -> affine1 -> relu1 -> affine2 -> relu2 -> affine3 -> softmax
  [W1, b1] = affine::init(D, 200, seed = seed)
  lseed = ifelse(seed==-1, -1, seed + 1);
  [W2, b2] = affine::init(200, 200,  seed = lseed)
  lseed = ifelse(seed==-1, -1, seed + 2);
  [W3, b3] = affine::init(200, K, seed = lseed)
  W3 = W3 / sqrt(2) # different initialization, since being fed into softmax, instead of relu


  # Create the model list
  model_list = list(W1, W2, W3, b1, b2, b3)
  # Create the hyper parameter list
  params = list(learning_rate=learning_rate)
  # Use paramserv function
  # model_trained = list(W1,W2,W3,b1,b2,b3)
  model_trained = paramserv(model=model_list, features=X, labels=y, val_features=X_val,
    val_labels=y_val,
    upd="./code/exp/FNN_network.dml::gradients", agg="./code/exp/FNN_network.dml::aggregation",
    mode=mode, freq=freq, epochs=epochs, batchsize=batch_size,
    k=workers, hyperparams=params, checkpointing="NONE")
}

predict = function(matrix[double] X, list[unknown] model)
    return (matrix[double] probs) {

  W1 = as.matrix(model[1])
  W2 = as.matrix(model[2])
  W3 = as.matrix(model[3])
  b1 = as.matrix(model[4])
  b2 = as.matrix(model[5])
  b3 = as.matrix(model[6])

  out1relu = relu::forward(affine::forward(X, W1, b1))
  out2relu = relu::forward(affine::forward(out1relu, W2, b2))
  probs = softmax::forward(affine::forward(out2relu, W3, b3))
}


eval = function(matrix[double] probs, matrix[double] y)
    return (double loss, double accuracy) {

  loss = cross_entropy_loss::forward(probs, y)
  accuracy = loss
}


gradients = function(list[unknown] model,
                     list[unknown] hyperparams,
                     matrix[double] features,
                     matrix[double] labels)
    return (list[unknown] gradients) {

  W1 = as.matrix(model[1])
  W2 = as.matrix(model[2])
  W3 = as.matrix(model[3])
  b1 = as.matrix(model[4])
  b2 = as.matrix(model[5])
  b3 = as.matrix(model[6])

  # Compute forward pass
  ## input -> affine1 -> relu1 -> affine2 -> relu2 -> affine3 -> softmax
  out1 = affine::forward(features, W1, b1)
  out1relu = relu::forward(out1)
  out2 = affine::forward(out1relu, W2, b2)
  out2relu = relu::forward(out2)
  out3 = affine::forward(out2relu, W3, b3)
  probs = softmax::forward(out3)

  # Compute loss & accuracy for training data
  loss = cross_entropy_loss::forward(probs, labels)
  accuracy = mean(rowIndexMax(probs) == rowIndexMax(labels))
  # print("[+] Completed forward pass on batch: train loss: " + loss + ", train accuracy: " + accuracy)

  # Compute data backward pass
  dprobs = cross_entropy_loss::backward(probs, labels)
  dout3 = softmax::backward(dprobs, out3)
  [dout2relu, dW3, db3] = affine::backward(dout3, out2relu, W3, b3)
  dout2 = relu::backward(dout2relu, out2)
  [dout1relu, dW2, db2] = affine::backward(dout2, out1relu, W2, b2)
  dout1 = relu::backward(dout1relu, out1)
  [dfeatures, dW1, db1] = affine::backward(dout1, features, W1, b1)

  gradients = list(dW1, dW2, dW3, db1, db2, db3)
}


aggregation = function(list[unknown] model,
                       list[unknown] hyperparams,
                       list[unknown] gradients)
    return (list[unknown] model_result) {

  W1 = as.matrix(model[1])
  W2 = as.matrix(model[2])
  W3 = as.matrix(model[3])
  b1 = as.matrix(model[4])
  b2 = as.matrix(model[5])
  b3 = as.matrix(model[6])
  dW1 = as.matrix(gradients[1])
  dW2 = as.matrix(gradients[2])
  dW3 = as.matrix(gradients[3])
  db1 = as.matrix(gradients[4])
  db2 = as.matrix(gradients[5])
  db3 = as.matrix(gradients[6])
  learning_rate = as.double(as.scalar(hyperparams["learning_rate"]))

  # Optimize with SGD
  W3 = sgd::update(W3, dW3, learning_rate)
  b3 = sgd::update(b3, db3, learning_rate)
  W2 = sgd::update(W2, dW2, learning_rate)
  b2 = sgd::update(b2, db2, learning_rate)
  W1 = sgd::update(W1, dW1, learning_rate)
  b1 = sgd::update(b1, db1, learning_rate)

  model_result = list(W1, W2, W3, b1, b2, b3)
}

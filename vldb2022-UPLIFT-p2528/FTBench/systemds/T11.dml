batchProcessing = function(Frame[Unknown] data, Frame[Unknown] meta, Matrix[Double] W) 
return(Double timer)
{
  # Approach1: RC col1 and pass it to a ctable
  # Approach2: DC col1. DC is equivelant of RC + ctable
  #jspec1 = "{ ids:true, recode:[1], dummycode:[1]}"; #DC 1st column
  jspec1 = "{ ids:true, recode:[1]}"; #RC 1st column
  maxLen = 1000;
  batchSize = 10000 * maxLen;
  maxiter = ceil(nrow(data)/batchSize);
  iter = 0;
  beg = 1;
  timer = 0

# Mini-batch processing
  print("INFO: starting batch transformapply");
  while (iter < maxiter) {
    end = beg + batchSize-1;
    batch = data[beg:end,];
    t1 = time();
    # Use the externally built dictionary to transformapply
    X_enc = transformapply(target=batch, spec=jspec1, meta=meta);
    X = table(seq(1,nrow(X_enc)), X_enc[,1]); #Approach1 with RC
    # Construct the embedding matrix (X axis -> abstracts)
    embd =  matrix(X%*%W, rows=nrow(X)/maxLen, cols=maxLen*300);
    print("("+nrow(embd)+", "+ncol(embd)+")");
    t2 = time();
    timer = timer + floor((t2-t1)/1000000);
    iter = iter + 1;
    beg = end + 1;
  }
}

# Set the maxLen in this script according to the abstract sequence 
# Read the sequence-of-words and the dictionary
data = read("../../datasets/AminerAbstractSequence.csv", data_type="frame", format="csv",header=FALSE);
meta = read("../../datasets/wiki_metaframe",
    data_type="frame", format="csv",sep="--",header=FALSE);
# Read the pre-trained model
W = read("../../datasets/wiki_embeddings", header=FALSE);
# Initiate reads
print(toString(data[1,1]));
print(toString(meta[1:10,]));
chunk = W[nrow(W)-5:nrow(W),1:5];
print(toString(chunk));

lim = 5;
R = matrix(0, rows=lim, cols=1);
for (i in 1:lim) {
  tot = batchProcessing(data, meta, W);
  R[i,1] = tot;
  print(tot);
}

res = order(target=R, by=1);
res = res[1:3,];
print(toString(res));
write(res, "embedding_dml.dat", format="csv", sep="\t");


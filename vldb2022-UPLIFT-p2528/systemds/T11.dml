# Set the maxLen in this script according to the abstract sequence 
lim = 5;
R = matrix(0, rows=lim, cols=1);
t1 = time();
# Read the sequence-of-words and the dictionary
data = read("file:/home/aphani/datasets/AminerAbstractSequence.csv", data_type="frame", format="csv",header=FALSE);
meta = read("file:/home/aphani/datasets/wiki_embeddings/wiki_metaframe",
    data_type="frame", format="csv",sep="--",header=FALSE);
# Read the pre-trained model
W = read("file:/home/aphani/datasets/wiki_embeddings/wiki_embeddings", header=FALSE);
# Initiate reads
print(toString(data[1,1]));
print(toString(meta[1:10,]));
chunk = W[nrow(W)-5:nrow(W),1:5];
print(toString(chunk));

# Approach1: RC col1 and pass it to a ctable
# Approach2: DC col1. DC is equivelant of RC + ctable
#jspec1 = "{ ids:true, recode:[1], dummycode:[1]}"; #DC 1st column
jspec1 = "{ ids:true, recode:[1]}"; #RC 1st column
t2 = time();
print("Elapsed time for reading = "+floor((t2-t1)/1000000)+" millsec");
maxLen = 1000;
batchSize = 10000 * maxLen;
maxiter = ceil(nrow(data)/batchSize);
iter = 0;
beg = 1;
timer = 0

# Mini-batch processing
while (iter < maxiter) {
  end = beg + batchSize-1;
  batch = data[beg:end,];
  print("INFO: starting transformapply");
  t1 = time();
  # Use the externally built dictionary to transformapply
  X_enc = transformapply(target=batch, spec=jspec1, meta=meta);
  print("("+nrow(X_enc)+", "+ncol(X_enc)+")");
  X = table(seq(1,nrow(X_enc)), X_enc[,1]); #Approach1 with RC
  # Construct the embedding matrix (X axis -> abstracts)
  embd =  matrix(X%*%W, rows=nrow(X)/maxLen, cols=maxLen*300);
  print("("+nrow(embd)+", "+ncol(embd)+")");
  t2 = time();
  print("Elapsed time for transformapply + ctable = "+floor((t2-t1)/1000000)+" millsec");
  timer = timer + floor((t2-t1)/1000000);
  #R[i,1] = R[i,1] + floor((t2-t1)/1000000);
  iter = iter + 1;
  beg = end + 1;
}
print("Elapsed time for all batches = "+timer+" millsec");


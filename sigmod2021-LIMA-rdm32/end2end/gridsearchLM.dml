l2norm = function(Matrix[Double] X, Matrix[Double] y, Matrix[Double] B, Integer icpt) 
return (Matrix[Double] loss) {
  if (icpt > 0)
    X = cbind(X, matrix(1, nrow(X), 1));
  loss = as.matrix(sum((y - X%*%B)^2));
}

randColSet = function(Matrix[Double] X, Integer seed, Double sample) return (Matrix[Double] Xi) {
  temp = rand(rows=ncol(X), cols=1, min = 0, max = 1, sparsity=1, seed=seed) <= sample
  Xi = removeEmpty(target = X, margin = "cols", select = temp);
}

###########################################

# Create datasets
M = $1;
X = rand(rows=$1, cols=100, sparsity=1.0, seed=1);
y = rand(rows=$1, cols=1, sparsity=1.0, seed=1);

Rbeta = matrix(0, rows=900, cols=ncol(X)); #nrows = 10*6*3*5 = 900
Rloss = matrix(0, rows=900, cols=1);
k = 1;

# Start 10 iterations
for (i in 1:10)
{
  # Randomly select 15% columns in every iteration
  Xi = randColSet(X, i, 0.15);

  # Start grid search hyper-parameter tuning for linear regression
  for (h1 in -5:0) {       #regularization - values:10^-5 to 10^0
    for (h2 in 0:2) {      #intercept - range: 0, 1, 2
      for (h3 in -12:-8) { #tolerance - values: 10^-12 to 10^-8
        reg = 10^h1;
        icpt = h2;
        tol = 10^h3;
        beta = lm(X=Xi, y=y, icpt=icpt, reg=reg, tol=tol, maxi=0, verbose=FALSE);
        Rbeta[k, 1:nrow(beta)] = t(beta);
        Rloss[k,] = l2norm(Xi, y, beta, icpt);
        k = k + 1;
      }
    }
  }
}

while(FALSE) {}
# Calculate the best model
leastLoss = rowIndexMin(t(Rloss));
bestModel = Rbeta[as.scalar(leastLoss),];
write(bestModel, "outdml", format="binary");
